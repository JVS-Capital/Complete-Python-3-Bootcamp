{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JVS-Capital/Complete-Python-3-Bootcamp/blob/master/Google_News_scraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update -qq\n",
        "!apt-get install -y python3-dev libxml2-dev libxslt1-dev libjpeg-dev zlib1g-dev libpng-dev\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXakJmolbG2o",
        "outputId": "981c63ba-a0c8-4580-c1f6-29301fc0fd77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libjpeg-dev is already the newest version (8c-2ubuntu10).\n",
            "libjpeg-dev set to manually installed.\n",
            "libpng-dev is already the newest version (1.6.37-3build5).\n",
            "libxml2-dev is already the newest version (2.9.13+dfsg-1ubuntu0.3).\n",
            "python3-dev is already the newest version (3.10.6-1~22.04).\n",
            "python3-dev set to manually installed.\n",
            "zlib1g-dev is already the newest version (1:1.2.11.dfsg-2ubuntu9.2).\n",
            "zlib1g-dev set to manually installed.\n",
            "The following NEW packages will be installed:\n",
            "  libxslt1-dev\n",
            "0 upgraded, 1 newly installed, 0 to remove and 8 not upgraded.\n",
            "Need to get 219 kB of archives.\n",
            "After this operation, 2,058 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libxslt1-dev amd64 1.1.34-4ubuntu0.22.04.1 [219 kB]\n",
            "Fetched 219 kB in 1s (423 kB/s)\n",
            "Selecting previously unselected package libxslt1-dev:amd64.\n",
            "(Reading database ... 129824 files and directories currently installed.)\n",
            "Preparing to unpack .../libxslt1-dev_1.1.34-4ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libxslt1-dev:amd64 (1.1.34-4ubuntu0.22.04.1) ...\n",
            "Setting up libxslt1-dev:amd64 (1.1.34-4ubuntu0.22.04.1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install newspaper3k\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_xmeKJUbIgA",
        "outputId": "3f1b1aa5-d3bb-4ccc-c487-b6f47d300f20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting newspaper3k\n",
            "  Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/211.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (4.11.2)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (8.4.0)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (6.0.1)\n",
            "Collecting cssselect>=0.9.2 (from newspaper3k)\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (4.9.3)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (3.8.1)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (2.27.1)\n",
            "Collecting feedparser>=5.2.1 (from newspaper3k)\n",
            "  Downloading feedparser-6.0.10-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tldextract>=2.0.1 (from newspaper3k)\n",
            "  Downloading tldextract-3.4.4-py3-none-any.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.3/93.3 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting feedfinder2>=0.0.4 (from newspaper3k)\n",
            "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jieba3k>=0.35.1 (from newspaper3k)\n",
            "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m123.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (2.8.2)\n",
            "Collecting tinysegmenter==0.3 (from newspaper3k)\n",
            "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.16.0)\n",
            "Collecting sgmllib3k (from feedparser>=5.2.1->newspaper3k)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (8.1.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (1.3.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (4.65.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (3.4)\n",
            "Collecting requests-file>=1.4 (from tldextract>=2.0.1->newspaper3k)\n",
            "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.12.2)\n",
            "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13541 sha256=5d4f865c1ce5c0cf9046d72fcf548b6366f96033efcf3838a8adc1f6bed734d8\n",
            "  Stored in directory: /root/.cache/pip/wheels/c8/d6/6c/384f58df48c00b9a31d638005143b5b3ac62c3d25fb1447f23\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3341 sha256=2f22c5fa4a840807a7e04a5a1ab6f65dcc8ab8a3b3df135b21017705ae02f016\n",
            "  Stored in directory: /root/.cache/pip/wheels/97/02/e7/a1ff1760e12bdbaab0ac824fae5c1bc933e41c4ccd6a8f8edb\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398382 sha256=e5f23d2682429ba03cc398ee445f08bcfd158d0e5cd54326c9058c2867649d04\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/c4/0c/12a9a314ecac499456c4c3b2fcc2f635a3b45a39dfbd240299\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6046 sha256=be2747c1be5bf5592c2e70774c39a74ab4a79700b622d893ea8f750f3e860635\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n",
            "Installing collected packages: tinysegmenter, sgmllib3k, jieba3k, feedparser, cssselect, requests-file, feedfinder2, tldextract, newspaper3k\n",
            "Successfully installed cssselect-1.2.0 feedfinder2-0.0.4 feedparser-6.0.10 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-1.5.1 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-3.4.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from newspaper import Article\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import time\n",
        "import os\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "h6Mpg8pBSZgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the base URL for the Google News search.\n",
        "URL = \"https://www.google.com/search?q=bitcoin+cryptocurrency&hl=en&gl=us&as_drrb=b&tbas=0&tbs=cdr:1,cd_min:{min_date},cd_max:{max_date},sbd:1&tbm=nws&sxsrf=ACYBGNRfmviSo9arK1e_P_YIl5wsskZBPw:1574225634362&source=lnt&sa=X&ved=0ahUKEwj4wu29__flAhWV9Z4KHaKJAGcQpwUIIA&biw=1685&bih=863&dpr=1.1\"\n",
        "\n",
        "# Define the headers to be used in the HTTP request.\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/46.0.2490.80 Safari/537.36',\n",
        "    'Content-Type': 'text/html',\n",
        "}\n",
        "\n",
        "# Define the maximum number of news articles to scrape per day.\n",
        "max_count = 10\n",
        "\n",
        "# Define the date format for displaying the scraped dates.\n",
        "fmt = '%m/%d/%Y'\n",
        "\n",
        "# Define the column names for the DataFrame to store the scraped news data.\n",
        "news_cols = ['index', 'date', 'status_code', 'url', 'news_1_url', 'news_1_text',\n",
        "             'news_1_publish_date', 'news_2_url', 'news_2_text', 'news_2_publish_date',\n",
        "             'news_3_url', 'news_3_text', 'news_3_publish_date', 'news_4_url',\n",
        "             'news_4_text', 'news_4_publish_date', 'news_5_url', 'news_5_text',\n",
        "             'news_5_publish_date', 'news_6_url', 'news_6_text', 'news_6_publish_date',\n",
        "             'news_7_url', 'news_7_text', 'news_7_publish_date', 'news_8_url',\n",
        "             'news_8_text', 'news_8_publish_date', 'news_9_url', 'news_9_text',\n",
        "             'news_9_publish_date']\n"
      ],
      "metadata": {
        "id": "hCdvIdMcZooH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the base URL for the Google News search.\n",
        "URL = \"https://www.google.com/search?q=bitcoin+cryptocurrency&hl=en&gl=us&as_drrb=b&tbas=0&tbs=cdr:1,cd_min:{min_date},cd_max:{max_date},sbd:1&tbm=nws&sxsrf=ACYBGNRfmviSo9arK1e_P_YIl5wsskZBPw:1574225634362&source=lnt&sa=X&ved=0ahUKEwj4wu29__flAhWV9Z4KHaKJAGcQpwUIIA&biw=1685&bih=863&dpr=1.1\"\n",
        "\n",
        "# Define the headers to be used in the HTTP request.\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/46.0.2490.80 Safari/537.36',\n",
        "    'Content-Type': 'text/html',\n",
        "}\n",
        "\n",
        "# Define the maximum number of news articles to scrape per day.\n",
        "max_count = 10\n",
        "\n",
        "# Define the date format for displaying the scraped dates.\n",
        "fmt = '%m/%d/%Y'\n",
        "\n",
        "# Define the column names for the DataFrame to store the scraped news data.\n",
        "news_cols = ['index', 'date', 'status_code', 'url', 'news_1_url', 'news_1_text',\n",
        "             'news_1_publish_date', 'news_2_url', 'news_2_text', 'news_2_publish_date',\n",
        "             'news_3_url', 'news_3_text', 'news_3_publish_date', 'news_4_url',\n",
        "             'news_4_text', 'news_4_publish_date', 'news_5_url', 'news_5_text',\n",
        "             'news_5_publish_date', 'news_6_url', 'news_6_text', 'news_6_publish_date',\n",
        "             'news_7_url', 'news_7_text', 'news_7_publish_date', 'news_8_url',\n",
        "             'news_8_text', 'news_8_publish_date', 'news_9_url', 'news_9_text',\n",
        "             'news_9_publish_date']\n"
      ],
      "metadata": {
        "id": "rS5z0BhHbefp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to scrape Google News for a specific date and save the data to a CSV file\n",
        "def google_news_scrapper(start_date, end_date, output_file_name):\n",
        "    \"\"\"\n",
        "    Scrape Google News articles for a range of dates and save the data to a CSV file.\n",
        "\n",
        "    Parameters:\n",
        "        start_date (str): The start date for scraping in the format 'MM/DD/YYYY'.\n",
        "        end_date (str): The end date for scraping in the format 'MM/DD/YYYY'.\n",
        "        output_file_name (str): The name of the CSV file to save the scraped data.\n",
        "    \"\"\"\n",
        "    # Convert the date strings to datetime objects and set the step for iterating through dates\n",
        "    step_obj = datetime.timedelta(days=1)\n",
        "    start_date_time_obj = datetime.datetime.strptime(start_date, fmt)\n",
        "    end_date_time_obj = datetime.datetime.strptime(end_date, fmt)\n",
        "\n",
        "    # Loop through the dates within the specified range\n",
        "    while start_date_time_obj <= end_date_time_obj:\n",
        "        # Convert the current date to string in 'MM/DD/YYYY' format\n",
        "        start_date = start_date_time_obj.strftime(fmt)\n",
        "        print(start_date)\n",
        "\n",
        "        # Run the Google News scrapper function for the current date and save the data\n",
        "        run_google_news_scrapper(min_date=start_date, max_date=start_date, output_file=output_file_name)\n",
        "\n",
        "        # Add a random delay between 2 to 5 seconds to avoid overwhelming the server\n",
        "        time.sleep(np.random.randint(2, 5))\n",
        "\n",
        "        # Move to the next date\n",
        "        start_date_time_obj += step_obj\n",
        "\n",
        "\n",
        "def sort_news_report(input_file_name, cleaned_output_file_name, save_index=False):\n",
        "    \"\"\"\n",
        "    Sort and clean a news report file, removing duplicate entries, and optionally save the index to a separate CSV file.\n",
        "\n",
        "    Parameters:\n",
        "        input_file_name (str): The name of the input CSV file containing the news report data.\n",
        "        cleaned_output_file_name (str): The name of the CSV file to save the sorted and cleaned data.\n",
        "        save_index (bool): If True, save the index to a separate CSV file. Default is False.\n",
        "    \"\"\"\n",
        "    # Read the news report data from the input file into a DataFrame\n",
        "    df = pd.read_csv(input_file_name)\n",
        "\n",
        "    # Set the 'date' column as the index and convert it to a datetime object\n",
        "    df = df.set_index('date', drop=True)\n",
        "    df.index = pd.to_datetime(df.index)\n",
        "\n",
        "    # Sort the DataFrame by the index (date) and drop duplicate entries, keeping the first occurrence\n",
        "    df = df.sort_index().drop_duplicates(keep='first')\n",
        "\n",
        "    # Save the sorted and cleaned data to the output file\n",
        "    df.to_csv(cleaned_output_file_name)\n",
        "\n",
        "    # If specified, save the index to a separate CSV file\n",
        "    if save_index:\n",
        "        df_i = pd.DataFrame(df.index)\n",
        "        df_i.to_csv(cleaned_output_file_name[0:-4] + '_index.csv')\n",
        "\n",
        "\n",
        "def clean_news_report(input_file_name, cleaned_output_file_name, save_index=False):\n",
        "    \"\"\"\n",
        "    Clean a news report file by removing unwanted columns and duplicate entries and optionally save the index.\n",
        "\n",
        "    Parameters:\n",
        "        input_file_name (str): The name of the input CSV file containing the news report data.\n",
        "        cleaned_output_file_name (str): The name of the CSV file to save the cleaned data.\n",
        "        save_index (bool): If True, save the index to a separate CSV file. Default is False.\n",
        "    \"\"\"\n",
        "    # Read the news report data from the input file into a DataFrame\n",
        "    master_df = pd.read_csv(input_file_name)\n",
        "\n",
        "    # Keep only the desired columns in the DataFrame\n",
        "    master_df = master_df[['date', 'news_1_text', 'news_2_text', 'news_3_text', 'news_4_text', 'news_5_text', 'news_6_text',\n",
        "             'news_7_text', 'news_8_text', 'news_9_text']]\n",
        "\n",
        "    # Set the 'date' column as the index and convert it to a datetime object\n",
        "    master_df = master_df.set_index('date', drop=True)\n",
        "    master_df.index = pd.to_datetime(master_df.index, format=fmt)\n",
        "\n",
        "    # Sort the DataFrame by the index (date) and drop duplicate entries, keeping the first occurrence\n",
        "    master_df = master_df.sort_index().drop_duplicates(keep='first')\n",
        "    idx = np.unique(master_df.index, return_index=True)[1]\n",
        "    master_df = master_df.iloc[idx]\n",
        "\n",
        "    # Save the cleaned data to the output file\n",
        "    master_df.to_csv(cleaned_output_file_name)\n",
        "\n",
        "    # If specified, save the index to a separate CSV file\n",
        "    if save_index:\n",
        "        df_i = pd.DataFrame(master_df.index)\n",
        "        df_i.to_csv(cleaned_output_file_name[0:-4] + '_index.csv')\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Define the start and end dates for scraping news articles\n",
        "    start_date = '07/01/2023'\n",
        "    end_date = '07/22/2023'\n",
        "\n",
        "    # Define the filename\n"
      ],
      "metadata": {
        "id": "VeWHhF7Sbuk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "    print(\"Scraping news articles from\", start_date, \"to\", end_date)\n",
        "\n",
        "    # Run the Google News scrapper function to scrape articles for the date range\n",
        "    news_raw_filename = 'google_news_final.csv'\n",
        "    google_news_scrapper(start_date, end_date, news_raw_filename)\n",
        "\n",
        "    # Clean and sort the news report data\n",
        "    news_cleaned_filename = news_raw_filename[0:-4] + '_cleaned.csv'\n",
        "    clean_news_report(news_raw_filename, news_cleaned_filename)\n",
        "\n",
        "    print(\"News articles scraped and saved successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQ5keq0Gir_5",
        "outputId": "35c1b7bc-971e-46de-ee01-226fe7a961fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping news articles from 07/01/2023 to 07/22/2023\n",
            "07/01/2023\n",
            "07/02/2023\n",
            "07/03/2023\n",
            "07/04/2023\n",
            "07/05/2023\n",
            "07/06/2023\n",
            "07/07/2023\n",
            "07/08/2023\n",
            "07/09/2023\n",
            "07/10/2023\n",
            "07/11/2023\n",
            "07/12/2023\n",
            "07/13/2023\n",
            "07/14/2023\n",
            "07/15/2023\n",
            "07/16/2023\n",
            "07/17/2023\n",
            "07/18/2023\n",
            "07/19/2023\n",
            "07/20/2023\n",
            "07/21/2023\n",
            "07/22/2023\n",
            "News articles scraped and saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kVRVtfUDb4rY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}